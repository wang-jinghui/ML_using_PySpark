{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('IO').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read txt\n",
    "\n",
    "- textFile : only text content\n",
    "- wholeTextFiles : file path and text content\n",
    "- 通用的三个参数:\n",
    "    - path\n",
    "    - minPartitions\n",
    "    - use_unicode : default False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sparkContext.textFile('./chapter6/dataFiles/shakespearePlays.txt', 2)\n",
    "\n",
    "data_list = data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Love's Labour's Lost     \",\n",
       " \"A Midsummer Night's Dream\",\n",
       " 'Much Ado About Nothing',\n",
       " 'As You Like It']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 字符总长度\n",
    "char_lenght = data.map(lambda x: len(x))\n",
    "\n",
    "char_lenght.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = spark.sparkContext.wholeTextFiles('./chapter6/dataFiles/shakespearePlays.txt',2)\n",
    "\n",
    "whole_data_list = whole_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/wjh/PYspark/pyspark-recipes/code_mishra/chapter6/dataFiles/shakespearePlays.txt',\n",
       "  \"Love's Labour's Lost     \\nA Midsummer Night's Dream\\nMuch Ado About Nothing\\nAs You Like It\\n\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file:/home/wjh/PYspark/pyspark-recipes/code_mishra/chapter6/dataFiles/shakespearePlays.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_data.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Love's Labour's Lost     \\nA Midsummer Night's Dream\\nMuch Ado About Nothing\\nAs You Like It\\n\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_data.values().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sparkContext.textFile('./chapter6/dataFiles/shakespearePlays.txt',4)\n",
    "dataLineLength = data.map(lambda x : len(x))\n",
    "\n",
    "dataLineLength.saveAsTextFile('./chapter6/dataFiles/save_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pyspark\n",
    "# Reading a directory using textFile() function.\n",
    "\n",
    ">>> manyFilePlayData = sc.textFile('/home/pysparkbook/pysparkBookData/manyFiles',4)\n",
    "\n",
    ">>> manyFilePlayData.collect()\n",
    "\n",
    "# Reading a directory using wholeTextFiles() function.\n",
    "\n",
    ">>> manyFilePlayDataKeyValue = sc.wholeTextFiles('/home/pysparkbook/pysparkBookData/manyFiles',4)\n",
    ">>> manyFilePlayDataKeyValue.collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Reading  Data from HDFS.\n",
    " \n",
    "\n",
    ">>> filamentdata = sc.textFile('hdfs://localhost:9746/bookData/filamentData.csv',4)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> playData = sc.textFile('/home/muser/bData/shakespearePlays.txt',4)\n",
    ">>> playDataLineLength = playData.map(lambda x : len(x))\n",
    ">>> playDataLineLength.collect()\n",
    "\n",
    "#  Saving RDD to HDFS.\n",
    "\n",
    ">>> playDataLineLength.saveAsTextFile('hdfs://localhost:9746/savedData/')\n",
    "\n",
    "\n",
    "###  hadoop fs -cat /savedData/part-00000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data from Sequential File\n",
    "\n",
    "```\n",
    "What is a sequential file?\n",
    "A sequential file is one whose contents is stored in some order. It must always be read starting from the beginning of the file. This is opposed to a direct access file, whose contents can be retrieved without reading the entire file.\n",
    "```\n",
    "\n",
    "- seqenceFile():\n",
    "   - path\n",
    "   - keyClass :  indicates the key class of data in the sequence file\n",
    "   - valueClass  datatype of the values\n",
    "\n",
    "```python \n",
    ">>> simpleRDD = sc.sequenceFile('hdfs://localhost:9746/sequenceFileToRead')\n",
    ">>> simpleRDD.collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write Data to a Sequential File "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pyhotn\n",
    "Data = [('si1','Python'),\n",
    "        ('si3','Java'),\n",
    "        ('si1','Java'),\n",
    "        ('si2','Python'),\n",
    "        ('si3','Ruby'),\n",
    "        ('si4','C++'),\n",
    "        ('si5','C'),\n",
    "        ('si4','Python'),\n",
    "        ('si2','Java')]\n",
    "\n",
    "\n",
    "RDD = sc.parallelize(subjectsData, 4)\n",
    "\n",
    "RDD.saveAsSequenceFile('hdfs://localhost:9746/sequenceFiles')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['p', 's', 'r', 'p']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import  io\n",
    "def parseCSV(csvRow) :\n",
    "    data = io.StringIO(csvRow)\n",
    "    dataReader =  csv.reader(data)\n",
    "    return([x for x in dataReader])\n",
    "\n",
    "csvRow = \"p,s,r,p\"\n",
    "parseCSV(csvRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['6AM', 15])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def jsonParse(dataLine):\n",
    "    parsedDict = json.loads(dataLine)\n",
    "    valueData = parsedDict.values()\n",
    "    return(valueData)\n",
    "\n",
    "jsonData = '{\"Time\":\"6AM\",  \"Temperature\":15}'\n",
    "jsonParsedData = jsonParse(jsonData)\n",
    "print(jsonParsedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> tempData = sc.textFile(\"/home/pysparkbook//pysparkBookData/tempData.json\",4)\n",
    "\n",
    ">>> tempData.take(4)\n",
    "\n",
    "# Creating paired RDD.\n",
    ">>> tempDataParsed = tempData.map(jsonParse)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Name\": \"Arun\", \"Age\": 22}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write json format\n",
    "def createJSON(data):\n",
    "    dataDict = {}\n",
    "    dataDict['Name'] = data[0]\n",
    "    dataDict['Age'] = data[1]\n",
    "    return(json.dumps(dataDict))\n",
    "\n",
    "nameAgeList = ['Arun',22]\n",
    "\n",
    "createJSON(nameAgeList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> nameAgeData = [['Arun',22],\n",
    "...                                  ['Bony',35],\n",
    "...                                  ['Juna',29]]\n",
    ">>> nameAgeRDD = sc.parallelize(nameAgeData,3)\n",
    "\n",
    ">>> nameAgeRDD.collect()\n",
    "\n",
    ">>> nameAgeJSON = nameAgeRDD.map(createJSON)\n",
    ">>> nameAgeJSON.collect()\n",
    ">>> nameAgeJSON.saveAsTextFile('/home/pysparkbook/jsonDir/')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading table data from HBase using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> hostName = 'localhost'\n",
    "\n",
    ">>> tableName = 'pysparkBookTable'\n",
    "\n",
    ">>> ourInputFormatClass='org.apache.hadoop.hbase.mapreduce.TableInputFormat'\n",
    ">>> ourKeyClass='org.apache.hadoop.hbase.io.ImmutableBytesWritable'\n",
    ">>> ourValueClass='org.apache.hadoop.hbase.client.Result'\n",
    ">>> ourKeyConverter='org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter'\n",
    ">>> ourValueConverter='org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter'\n",
    ">>> configuration = {}\n",
    ">>> configuration['hbase.mapreduce.inputtable'] = tableName\n",
    ">>> configuration['hbase.zookeeper.quorum'] = hostName\n",
    "\n",
    "Now it is time to call the function newAPIHadoopRDD() with its arguments. \n",
    "\n",
    ">>> tableRDDfromHBase = sc.newAPIHadoopRDD(\n",
    "...                        inputFormatClass = ourInputFormatClass,\n",
    "...                        keyClass = ourKeyClass,\n",
    "...                        valueClass = ourValueClass,\n",
    "...                        keyConverter = ourKeyConverter,\n",
    "...                        valueConverter = ourValueConverter,\n",
    "...                        conf = configuration\n",
    "...                     )\n",
    "\n",
    "\n",
    "Let us see how our paired RDD  tableRDDfromHBase looks like. \n",
    "\n",
    ">>> tableRDDfromHBase.take(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
